<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Wed, 03 Jan 2024 12:33:56 GMT</lastBuildDate>
    <item>
      <title>使用 open ai api 在我的应用程序中实现文件上传</title>
      <link>https://community.openai.com/t/implementing-a-file-upload-in-my-application-using-open-ai-api/575020#post_7</link>
      <description><![CDATA[如何执行 RAG 取决于您。你可以使用 OpenAI 的 ada-embeddings-002 模型，它非常便宜。或者，如果您愿意，您可以托管自己的嵌入模型。]]></description>
      <guid>https://community.openai.com/t/implementing-a-file-upload-in-my-application-using-open-ai-api/575020#post_7</guid>
      <pubDate>Wed, 03 Jan 2024 12:25:10 GMT</pubDate>
    </item>
    <item>
      <title>每次我向 chatGPT 提问时都会遇到“验证你是人类”的难题！</title>
      <link>https://community.openai.com/t/verify-you-are-human-puzzle-each-time-i-ask-a-question-to-chatgpt/455957?page=3#post_51</link>
      <description><![CDATA[天啊，我一直被这个愚蠢的验证人类搞疯了。如果你错过了一个，你就必须重新开始。
这是最烦人的人工验证！！！
当我用 pdf 文件发布问题后出现“网络错误”时，情况就开始了。没有 pdf 似乎可以工作。
日期/时间不是问题。
根据正常运行时间，OpenAi chatgpt 没有任何问题。 （我对此表示怀疑）
但是你能找到另一种人类检测​​来代替这些愚蠢的谜题吗，它们对人类来说太困难了。我需要编写一个机器人才能正确响应它们......（我可怜的眼睛）
我怀疑计算机无法解决这些问题......
我知道为什么 openai 让我们做这些谜题：我们正在免费训练一个识别模型。作为付费客户，我不接受这一点。
我对所有这些网络错误、简短而懒惰的 chatgpt 答案和愚蠢的谜题感到非常沮丧。
现在我还收到错误“对话太长，请开始新的对话。”对于只有一个句子的问题和一页单词 docx]]></description>
      <guid>https://community.openai.com/t/verify-you-are-human-puzzle-each-time-i-ask-a-question-to-chatgpt/455957?page=3#post_51</guid>
      <pubDate>Wed, 03 Jan 2024 12:23:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 open ai api 在我的应用程序中实现文件上传</title>
      <link>https://community.openai.com/t/implementing-a-file-upload-in-my-application-using-open-ai-api/575020#post_6</link>
      <description><![CDATA[如果我将RAG纳入我的项目并将其提交到服务器，服务器会产生额外的费用吗？我的要求是它必须分析文件中的数据，如 gpt-4 所示，并在我输入 csv 或 xlsx 格式的文档时在其中创建讨论。]]></description>
      <guid>https://community.openai.com/t/implementing-a-file-upload-in-my-application-using-open-ai-api/575020#post_6</guid>
      <pubDate>Wed, 03 Jan 2024 12:14:46 GMT</pubDate>
    </item>
    <item>
      <title>Assistant API Streaming 有更新吗？</title>
      <link>https://community.openai.com/t/any-updates-on-assistant-api-streaming/551809#post_11</link>
      <description><![CDATA[有关于何时支持流媒体的消息吗？我真的不想拼凑一个手动版本的助手！]]></description>
      <guid>https://community.openai.com/t/any-updates-on-assistant-api-streaming/551809#post_11</guid>
      <pubDate>Wed, 03 Jan 2024 12:03:16 GMT</pubDate>
    </item>
    <item>
      <title>大错误：GPT 3.5 和 GPT-4 的结果根本不一致</title>
      <link>https://community.openai.com/t/big-mistake-the-results-do-not-coincide-at-all-between-gpt-3-5-and-gpt-4/578154#post_1</link>
      <description><![CDATA[我尝试比较翻译文本的 2 个结果。您将看到差异（左 - GPT 3.5 和右 - GPT 4）
它们绝不相同。除了翻译都不是很好之外，似乎它们甚至都不重合。
这是一个很大的错误，并非巧合。我的意思是，如果我告诉 GPT 3.5 和 GPT 4 考虑 5+5 是多少，那么给我结果 10 和其他 15 是否正确？
这是一个很大的错误。两个版本的结果必须一致。否则你不知道哪一个是真的。
我知道专业版更好，但在插件、扩展和集成方面一定更好。但结果不允许有不同。

这是我翻译的文字：

Printre afectiunile in care acupunctura se dovedeste utila enumeram :

* dureri 肌肉骨关节（lombalgii、坐骨神经痛、斜颈、[artroza](https://www.cdt-babes.ro/articole/artroza.php) in diferitele ei forme - gonartroza、coxartroza、artroza umarului、spondiloza、 dureri de spate、de ceafa、solduri、brate）
* modificari Structuree mici de coloana vertebrala ([椎间盘疝气](https://www.cdt-babes.ro/articole/hernie-de-disc.php) incipienta)
* 在肠胃消化中：balonari、arsuri、hiperaciditate、dureri腹部、便秘、[diaree](https://www.cdt-babes.ro/articole/gastroenterita_diaree_estivala.php)
* sfera sistemului nervos : [焦虑](https://www.cdt-babes.ro/articole/anxietatea.php), [失眠](https://www.cdt-babes.ro/articole/igiena-somnului) .php), 神经植物综合症
* tulburari 内分泌 : dureri menstruale, menstre neregulate, infertilitate

在 bolile cronice 中，针灸 ajuta prin ameliorarea simptomatologiei，rarirea acutizarilor，reabilitare 功能。
]]></description>
      <guid>https://community.openai.com/t/big-mistake-the-results-do-not-coincide-at-all-between-gpt-3-5-and-gpt-4/578154#post_1</guid>
      <pubDate>Wed, 03 Jan 2024 12:00:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM(GPT-3.5-Turbo) 将自然语言转换为 SQL 查询</title>
      <link>https://community.openai.com/t/convert-natural-language-to-sql-query-using-llm-gpt-3-5-turbo/575015#post_6</link>
      <description><![CDATA[您使用的是哪种提示和模型参数？]]></description>
      <guid>https://community.openai.com/t/convert-natural-language-to-sql-query-using-llm-gpt-3-5-turbo/575015#post_6</guid>
      <pubDate>Wed, 03 Jan 2024 11:43:31 GMT</pubDate>
    </item>
    <item>
      <title>采用 gpt-4 模型的 open-ai 引擎是否会记住之前的提示令牌并在后续请求中再次使用它们进行响应？</title>
      <link>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_5</link>
      <description><![CDATA[目前还没有像您所描述的那样具有零成本内存的大型语言模型。它们都需要每次都包含全套说明。]]></description>
      <guid>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_5</guid>
      <pubDate>Wed, 03 Jan 2024 11:38:59 GMT</pubDate>
    </item>
    <item>
      <title>采用 gpt-4 模型的 open-ai 引擎是否会记住之前的提示令牌并在后续请求中再次使用它们进行响应？</title>
      <link>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_4</link>
      <description><![CDATA[


 类别：
&lt;块引用&gt;
有什么方法可以缓存第一个提示属性（例如大文本数据），并告诉 open-ai 使用该特定缓存来形成课程计划？


如果您正在管理自己的对话历史记录，那么您就走在正确的道路上。
但是，每次 API 调用都不可避免地向 AI 发出指令（除了非常巧妙的技术 - 几次高质量的聊天回合也对 AI 进行了一些训练，您可以尝试替换更少的指令）
在 AI 再次查看所有数据的情况下，也不会重复从数据中回答。
前言：如果你想控制成本，就不要使用“助手”
人工智能摘要和文档准备可以帮助您减少每次调用中持续存在的实际令牌数量。
然后，如果你“聊天”：
首先：计算每条消息和响应的令牌。添加此元数据对于管理和计算成本非常有用。
然后考虑应用程序实际上需要多少过去的转弯。
如果你需要延长聊天时间以产生长记忆的错觉，可以使用第二个人工智能偶尔进行的总结，甚至可以使用语义数据库来检索旧的聊天回合。
您可以减少用户输入的数量或助理输出的数量。对于聊天来说，用户输入的内容可能比长人工智能响应更有助于理解主题。对于编码，这可能相反。
数据传输和输入加载并不需要太多时间。 AI 可以在 2 秒内以流模式开始响应。然而，这种让人工智能做好回答准备的输入处理每次也需要计算资源。您不会被分配一个属于您自己的 AI 模型。]]></description>
      <guid>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_4</guid>
      <pubDate>Wed, 03 Jan 2024 11:38:18 GMT</pubDate>
    </item>
    <item>
      <title>采用 gpt-4 模型的 open-ai 引擎是否会记住之前的提示令牌并在后续请求中再次使用它们进行响应？</title>
      <link>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_3</link>
      <description><![CDATA[嗨@Foxabilo，
感谢您的及时回复。还有其他模型可以支持我们的要求吗？请指导我们优化令牌使用和响应时间。
谢谢。]]></description>
      <guid>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_3</guid>
      <pubDate>Wed, 03 Jan 2024 11:26:19 GMT</pubDate>
    </item>
    <item>
      <title>无法公开 GPT - 构建者配置文件中没有显示名称</title>
      <link>https://community.openai.com/t/can-not-make-gpt-public-no-name-shows-up-in-builder-profile/483870?page=5#post_84</link>
      <description><![CDATA[同样的问题！为了完成构建者配置文件，没有显示任何名称。有修复了吗？]]></description>
      <guid>https://community.openai.com/t/can-not-make-gpt-public-no-name-shows-up-in-builder-profile/483870?page=5#post_84</guid>
      <pubDate>Wed, 03 Jan 2024 11:25:43 GMT</pubDate>
    </item>
    <item>
      <title>采用 gpt-4 模型的 open-ai 引擎是否会记住之前的提示令牌并在后续请求中再次使用它们进行响应？</title>
      <link>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_2</link>
      <description><![CDATA[嗨，
不，该模型是无状态的，因此它需要每次传递所有信息。目前没有办法解决这个问题。]]></description>
      <guid>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_2</guid>
      <pubDate>Wed, 03 Jan 2024 11:15:37 GMT</pubDate>
    </item>
    <item>
      <title>助理 API 性能非常慢</title>
      <link>https://community.openai.com/t/assistant-api-performance-is-very-slow/557716#post_7</link>
      <description><![CDATA[让我们列一个清单，然后检查两次......
在我的图表中，成本是每100万个代币，因此您可以更轻松地比较价格：

&lt;表&gt;
&lt;标题&gt;

型号
训练 100 万
输入使用量1M
输出使用量1M
上下文长度


&lt;正文&gt;

GPT-3.5-turbo-1106
$n/a
1.00 美元
$2.00
16k


GPT-3.5-turbo
$n/a
1.50 美元
$2.00
4k


GPT-3.5-turbo-16k
$n/a
3.00 美元
$4.00
16k（4k 输出）


GPT-3.5 Turbo 微调
$8.00
3.00 美元
$6.00
4k


GPT-4-1106（涡轮）
$n/a
10.00 美元
$30.00
125k（4k 输出）


GPT-4
$n/a
30.00 美元
$60.00
8k


--------
-------------
-------------
-------------
-------------


babbage-002基地
$n/a
0.40 美元
0.40 美元
16k


babbage-002 微调
0.40 美元
1.60 美元
1.60 美元
16k


--------
-------------
-------------
-------------
-------------


达芬奇-002底座
$n/a
$2.00
$2.00
16k


davinci-002 微调
$6.00
$12.00
$12.00
16k



比较价格&lt; /h3&gt;

&lt;表&gt;
&lt;标题&gt;

令牌
gpt-3.5-turbo
gpt-4-turbo (1106)


&lt;正文&gt;

输入
1x
6.67x


输出
1x
15x



]]></description>
      <guid>https://community.openai.com/t/assistant-api-performance-is-very-slow/557716#post_7</guid>
      <pubDate>Wed, 03 Jan 2024 11:12:24 GMT</pubDate>
    </item>
    <item>
      <title>研究：通过 LCM 变压器避免 GPT 幻觉并实现 AGI</title>
      <link>https://community.openai.com/t/research-avoiding-gpts-hallucinations-and-achieving-agi-through-lcm-transformers/578149#post_1</link>
      <description><![CDATA[您可能是第一次遇到 LCM 这个术语，原因很简单：我刚刚创造了它。 LCM 代表“大型概念模型”。与 LLM 在句法层面使用原子神经元素作为单词不同，LCM 在语义层面使用原子神经元素作为概念。大约 15 年前，我写了两篇文章，如果今天重新审视，可能有助于避免幻觉并实现真正的 AGI，而不仅仅是其外表。
第一篇文章，位于 https://www.codeproject.com/…/The-Building-of-a…，介绍计算谓词微积分的基础知识。它解决了经常被忽视的问题：什么是概念？我们可以用它们执行什么操作？我们如何操纵和代表它们？这些问题一直被概念科学所忽视，概念科学传统上关注单词和字母等句法元素。为了实现我们对 AGI 的追求，我相信我们必须扩展我们当前的成就，并将最终的转换器纳入不同的 GPT，最终形成概念元素的神经网络。
第二篇文章，位于 https://www.codeproject.com/…/True-Natural-Language…，演示了借助“概念词典”从语法级别到语义级别的转换。它详细介绍了如何确定性地将句法形式转换为谓词（概念）以进行进一步的操作和操作。它还介绍了针对谓词知识库进行演绎和推理的概念和代码。
这些文章的另一个有趣的方面是它们与量子计算的交叉点，利用三值状态变量 (YES-NO-MAYBE) 而不是二进制状态变量，表明在追求 AGI 的过程中两次 IT 革命的潜在融合。
愿景
设想一个场景，其中 LCM 转换器集成到最先进且一致的 GPT 中，其中原语和名称-值谓词对等概念标记成为神经网络的一部分，而不是单词等句法元素。这种转变将使知识从根本上成为概念性的而非句法性的，从而更接近人类认知并有可能实现通用人工智能。这种方法还可以规避语言模型中固有的幻觉，因为它可以在公认的概念元素而不是句法元素的迷宫中导航。
十五年前，构建概念词典是一个重大障碍。然而，凭借当前 GPT 在处理编程语言方面的熟练程度，这一障碍可能是可以克服的。想象一下，利用 GPT 理解和操作代码的能力，在短短六个月内就可以生成包含整个英语词汇的综合概念词典。这一突破可以极大地加速我们迈向 AGI 的进程。]]></description>
      <guid>https://community.openai.com/t/research-avoiding-gpts-hallucinations-and-achieving-agi-through-lcm-transformers/578149#post_1</guid>
      <pubDate>Wed, 03 Jan 2024 11:12:21 GMT</pubDate>
    </item>
    <item>
      <title>采用 gpt-4 模型的 open-ai 引擎是否会记住之前的提示令牌并在后续请求中再次使用它们进行响应？</title>
      <link>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_1</link>
      <description><![CDATA[大家好，
一周以来，我们一直在使用 open-ai gpt-4 模型根据作为提示标记传递的文本数据生成结构化课程计划。现在每次我们都会通过巨大的提示让引擎为我们提供所需的信息。这每次都会消耗大量令牌，并且基本上耗尽了每个请求的令牌限制。甚至响应时间也超过了5分钟。有什么方法可以缓存第一个提示属性（例如大文本数据），并告诉 open-ai 使用该特定缓存来形成课程计划？这是为了缩短请求时间并降低令牌消耗，以便每天检索更多完成令牌。
谢谢。]]></description>
      <guid>https://community.openai.com/t/does-the-open-ai-engine-with-gpt-4-model-remember-the-previous-prompt-tokens-and-respond-using-them-again-in-subsequent-requests/578148#post_1</guid>
      <pubDate>Wed, 03 Jan 2024 11:09:16 GMT</pubDate>
    </item>
    <item>
      <title>自定义 GPT 消失了？！</title>
      <link>https://community.openai.com/t/custom-gpt-disappearing/546131?page=3#post_42</link>
      <description><![CDATA[更新&gt; GPT 在联系支持人员后回来了 ]]></description>
      <guid>https://community.openai.com/t/custom-gpt-disappearing/546131?page=3#post_42</guid>
      <pubDate>Wed, 03 Jan 2024 11:00:41 GMT</pubDate>
    </item>
    </channel>
</rss>