<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Wed, 27 Mar 2024 01:10:53 GMT</lastBuildDate>
    <item>
      <title>异步助手 API 流测试版</title>
      <link>https://community.openai.com/t/async-assistantapi-streaming-beta/698736#post_2</link>
      <description><![CDATA[我没有使用过异步 AssistantApi。主要是因为我正在使用 Flask，虽然我可以创建一个线程来运行 Aysncio，但该线程不与主 Flask 共享上下文（并且懒得弄清楚如何实现）。
也就是说，在 Flask 的上下文中，我必须在“for event in stream”中执行 eventlet.sleep(0)...ymmv]]></description>
      <guid>https://community.openai.com/t/async-assistantapi-streaming-beta/698736#post_2</guid>
      <pubDate>Wed, 27 Mar 2024 00:53:54 GMT</pubDate>
    </item>
    <item>
      <title>功能性聊天机器人最佳实践</title>
      <link>https://community.openai.com/t/functional-chat-bot-best-practices/613961#post_4</link>
      <description><![CDATA[这件事引起了我的注意。想知道是否有人尝试过其他方法？
假设您有可列出、可命名的线程和带有附加属性的消息，为什么不能将添加/删除的衣服存储到带有自定义消息的一个线程中？
库存消息：
Clothing_label…(“蓝色衬衫”)
Storage_location（“壁橱”）
操作（“添加/删除”）
有一个函数调用来触发这个吗？
其他一些问题：“什么衣服适合我朋友举办的毕业派对？”需要将当前所有衣服放在一个位置进行实际解释。]]></description>
      <guid>https://community.openai.com/t/functional-chat-bot-best-practices/613961#post_4</guid>
      <pubDate>Wed, 27 Mar 2024 00:47:42 GMT</pubDate>
    </item>
    <item>
      <title>Davinci-002微调完成不起作用</title>
      <link>https://community.openai.com/t/davinci-002-fine-tuning-completion-does-not-work/698691#post_3</link>
      <description><![CDATA[感谢您的帮助。

不幸的是，由于之前同一数据集上的空格，我也遇到过这样的问题，但我修复了它并且它适用于旧模型。所以我的问题可能与数据集本身无关，但无论如何我都会检查它。
较低的温度不会改变任何东西 
]]></description>
      <guid>https://community.openai.com/t/davinci-002-fine-tuning-completion-does-not-work/698691#post_3</guid>
      <pubDate>Wed, 27 Mar 2024 00:40:01 GMT</pubDate>
    </item>
    <item>
      <title>用户设置需要公开版本号（某种形式）？</title>
      <link>https://community.openai.com/t/user-settings-needs-to-expose-a-version-number-of-some-sort/695420#post_3</link>
      <description><![CDATA[与过去的结果相比，我的建议更多是基于我对功能较差的 ChatGPT 响应（即 GPT 构建器无法完成自定义 GPT 设置）的体验，以及关键功能的行为不同......因此我可能使用不同的版本（发布）给其他人，使用相同的界面。
自从发布上述内容后，我注意到我的网站会话登录再次发生了变化，回到了过去，更多以前使用过的自定义 GPT 显示在左侧边栏上，GPT 构建器（助理）的响应更好，但是仍然无法保存自定义 GPT（对我来说）。
所以，如果我知道我的版本/版本有一个已知问题，然后我就不用费心去尝试解决它或找出原因，这对我来说是有帮助的。或者至少知道我正在与其他人比较什么问题。]]></description>
      <guid>https://community.openai.com/t/user-settings-needs-to-expose-a-version-number-of-some-sort/695420#post_3</guid>
      <pubDate>Wed, 27 Mar 2024 00:30:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gpt 3 模型无法产生 2000 字的输出</title>
      <link>https://community.openai.com/t/why-gpt-3-model-failed-to-produce-2000-word-output/698720#post_9</link>
      <description><![CDATA[


 blogcutter：
&lt;块引用&gt;
我按照你说的做了


我们可以看一下步骤/提示的示例吗？]]></description>
      <guid>https://community.openai.com/t/why-gpt-3-model-failed-to-produce-2000-word-output/698720#post_9</guid>
      <pubDate>Wed, 27 Mar 2024 00:26:13 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gpt 3 模型无法产生 2000 字的输出</title>
      <link>https://community.openai.com/t/why-gpt-3-model-failed-to-produce-2000-word-output/698720#post_8</link>
      <description><![CDATA[chatgpt 中的聊天基础创建相同的响应也提供相同的基于 600-800 令牌的响应结果。]]></description>
      <guid>https://community.openai.com/t/why-gpt-3-model-failed-to-produce-2000-word-output/698720#post_8</guid>
      <pubDate>Wed, 27 Mar 2024 00:21:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gpt 3 模型无法产生 2000 字的输出</title>
      <link>https://community.openai.com/t/why-gpt-3-model-failed-to-produce-2000-word-output/698720#post_7</link>
      <description><![CDATA[我按照你说的做了，相同的输出，在基于大纲的提示中使用了最高tokken 853]]></description>
      <guid>https://community.openai.com/t/why-gpt-3-model-failed-to-produce-2000-word-output/698720#post_7</guid>
      <pubDate>Wed, 27 Mar 2024 00:20:33 GMT</pubDate>
    </item>
    <item>
      <title>异步助手 API 流测试版</title>
      <link>https://community.openai.com/t/async-assistantapi-streaming-beta/698736#post_1</link>
      <description><![CDATA[那里的例子并不多，但很好奇是否有人有幸以异步方式使用 Assistants API（测试版）将流推送到前端。
我的应用程序使用 python 编写，并使用 FastAPI 作为 BE 服务器。
响应需要一些时间才能完整发送回用户，我希望通过流式传输，用户至少会开始更快地获得响应。]]></description>
      <guid>https://community.openai.com/t/async-assistantapi-streaming-beta/698736#post_1</guid>
      <pubDate>Wed, 27 Mar 2024 00:19:01 GMT</pubDate>
    </item>
    </channel>
</rss>