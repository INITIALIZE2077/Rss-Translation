<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Tue, 09 Jan 2024 01:15:56 GMT</lastBuildDate>
    <item>
      <title>LLM 代理的最大痛点（Assistants API、Autogen 等）</title>
      <link>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_15</link>
      <description><![CDATA[我也不使用代理或框架。
我最后看的是BabyAGI。我只是采用了该代码，对其进行了简化，并将其命名为 CurtGPT 
不使用代理的一个重要原因是我认为我不需要它们。我已经可以做任何我想做的事情，只需自己编写代码即可。如果他们有我想要的功能，而我不知道如何去做，我会查看他们的代码并制作我自己的版本。
因此，代理用于了解实现，或者了解哪些工具可能对我为自己的用例构建有用。]]></description>
      <guid>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_15</guid>
      <pubDate>Tue, 09 Jan 2024 01:15:48 GMT</pubDate>
    </item>
    <item>
      <title>Dall-E-3 不会生成 1024x1024 以外尺寸的图像</title>
      <link>https://community.openai.com/t/dall-e-3-wont-produce-image-in-sizes-other-than-1024x1024/578773#post_5</link>
      <description><![CDATA[是的，完全相同的代码。只需粘贴“1792x1024”，然后是错误的请求。它与“1024x1024”完美配合。]]></description>
      <guid>https://community.openai.com/t/dall-e-3-wont-produce-image-in-sizes-other-than-1024x1024/578773#post_5</guid>
      <pubDate>Tue, 09 Jan 2024 01:00:13 GMT</pubDate>
    </item>
    <item>
      <title>Dall-E-3 不会生成 1024x1024 以外尺寸的图像</title>
      <link>https://community.openai.com/t/dall-e-3-wont-produce-image-in-sizes-other-than-1024x1024/578773#post_4</link>
      <description><![CDATA[调用生成 1024x1024 图像正常。所以它确实传递了模型参数。]]></description>
      <guid>https://community.openai.com/t/dall-e-3-wont-produce-image-in-sizes-other-than-1024x1024/578773#post_4</guid>
      <pubDate>Tue, 09 Jan 2024 00:59:23 GMT</pubDate>
    </item>
    <item>
      <title>自定义 GPT 中的超链接未链接？</title>
      <link>https://community.openai.com/t/hyperlinks-in-custom-gpt-not-linking/565252#post_21</link>
      <description><![CDATA[过去两天我在自己的 GPT 上多次遇到此问题。]]></description>
      <guid>https://community.openai.com/t/hyperlinks-in-custom-gpt-not-linking/565252#post_21</guid>
      <pubDate>Tue, 09 Jan 2024 00:56:37 GMT</pubDate>
    </item>
    <item>
      <title>按会话数排名前 500 名的 GPT</title>
      <link>https://community.openai.com/t/ranking-of-top-500-gpts-by-conversations/578607#post_5</link>
      <description><![CDATA[这个博客&lt; /a&gt; 解释它是如何完成的。
这些数字是按对话数量计算的，而不是按唯一用户数计算的。]]></description>
      <guid>https://community.openai.com/t/ranking-of-top-500-gpts-by-conversations/578607#post_5</guid>
      <pubDate>Tue, 09 Jan 2024 00:50:28 GMT</pubDate>
    </item>
    <item>
      <title>自定义 GPT 中的超链接未链接？</title>
      <link>https://community.openai.com/t/hyperlinks-in-custom-gpt-not-linking/565252#post_20</link>
      <description><![CDATA[您看到我上面的更新了吗？
问题已经解决。]]></description>
      <guid>https://community.openai.com/t/hyperlinks-in-custom-gpt-not-linking/565252#post_20</guid>
      <pubDate>Tue, 09 Jan 2024 00:37:48 GMT</pubDate>
    </item>
    <item>
      <title>LLM 代理的最大痛点（Assistants API、Autogen 等）</title>
      <link>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_14</link>
      <description><![CDATA[


某人Sysop：
&lt;块引用&gt;
以我的拙见，上述工具都无法解决在试图从法学硕士那里获得连贯、一致、全面、特别是廉价的答复时出现的无数问题。


你是对的。这就是我个人对当前代理框架的看法 - 它们适合某些任务，但不适用于需要细致控制的任何任务。]]></description>
      <guid>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_14</guid>
      <pubDate>Tue, 09 Jan 2024 00:26:32 GMT</pubDate>
    </item>
    <item>
      <title>LLM 代理的最大痛点（Assistants API、Autogen 等）</title>
      <link>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_13</link>
      <description><![CDATA[预测很难。让万花齐放。
就我个人而言，我认为法学硕士是有趣的语言处理器，可以围绕它们构建基于语言的图灵机，并且我用 python 从头开始​​围绕它们构建其他所有内容。但我是一名研究员。如果您正在尝试构建一个应用程序，并且任何抽象级别的现有框架都适合它，为什么不呢？例如，LangChain 对于许多简单的文档处理应用来说是“更少的代码”。
总的来说，恕我直言，宗教战争是愚蠢的。如果你喜欢一个工具，就使用它。如果您认为您的社区“拥有”一个术语（例如“无代码”），那么，好吧，当然，我很高兴地承认我的使用环境可能与您的不同。 ...
也许更有趣的是之前提出的核心主张：
如果一个人正在做任何前沿的事情，法学硕士不会知道，因此也无济于事。
是的！宾果游戏。
但恕我直言，下一代将包括实时 24/7 摄取新信息（如果不是通过增量训练，那么通过下一代 RAG 知识库的实时更新，（参见我的项目 Owl）。所以也许不是今天，但很快，机器人就会比你/我了解更多、更了解最新情况。
现在怎么办？]]></description>
      <guid>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_13</guid>
      <pubDate>Tue, 09 Jan 2024 00:18:48 GMT</pubDate>
    </item>
    <item>
      <title>相同提示的结果不一致</title>
      <link>https://community.openai.com/t/inconsistent-results-with-identical-prompts/578844#post_2</link>
      <description><![CDATA[您实际上是否提供了其他数据，例如检索数据库或可以查找地址的外部函数？
如果不提供外部数据源，这完全不是人工智能能够可靠回答的问题类型，除非地址是宾夕法尼亚大道 1600 号（您会注意到确实会产生每次都能得到可靠的答案...）
产生令人信服但不真实的信息被称为“幻觉”；人工智能只会创造看起来是回答此类问题的最佳方式的语言。
要为相同的输入获得相同的输出，测试生成的最可靠方法是使用 top-p API 参数，设置 top_p:0.001。这确保了只能生成概率最高的 0.1% 答案中的标记，从而产生更具可重复性的输出。
更适度的设置（例如 0.5）也可以提高人工智能训练中不太常见的语言的质量。]]></description>
      <guid>https://community.openai.com/t/inconsistent-results-with-identical-prompts/578844#post_2</guid>
      <pubDate>Tue, 09 Jan 2024 00:10:34 GMT</pubDate>
    </item>
    <item>
      <title>按会话数排名前 500 名的 GPT</title>
      <link>https://community.openai.com/t/ranking-of-top-500-gpts-by-conversations/578607#post_4</link>
      <description><![CDATA[如果目前显然只有几十万 chatgpt plus 订阅者，那么我并不完全理解这些数字，除非它们被少数人多次使用。]]></description>
      <guid>https://community.openai.com/t/ranking-of-top-500-gpts-by-conversations/578607#post_4</guid>
      <pubDate>Tue, 09 Jan 2024 00:10:29 GMT</pubDate>
    </item>
    <item>
      <title>相同提示的结果不一致</title>
      <link>https://community.openai.com/t/inconsistent-results-with-identical-prompts/578844#post_1</link>
      <description><![CDATA[嗨，我是新手，请耐心等待。我编写了一个 python 脚本来询问 chatgtp 某个地址是否位于保加利亚的住宅区或商业区。我得到的回复说它是商业、住宅或不适用，这意味着它无法确认它是商业还是住宅。一天后再次运行相同的地址时，它给了我不同的结果，现在主要是说这些地址属于未知类别。
提示：улица Васил Левски 27А Велико Търново 保加利亚是住宅区还是商业区？回答商业还是住宅，然后总结
当我在网络界面中执行提示时，我总是得到相同的一致结果。
我应该调整温度还是需要调整提示？
谢谢！！！]]></description>
      <guid>https://community.openai.com/t/inconsistent-results-with-identical-prompts/578844#post_1</guid>
      <pubDate>Tue, 09 Jan 2024 00:02:40 GMT</pubDate>
    </item>
    <item>
      <title>LLM 代理的最大痛点（Assistants API、Autogen 等）</title>
      <link>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_12</link>
      <description><![CDATA[


 托尼AIChamp：
&lt;块引用&gt;
我很好奇您认为使用 LLM 代理框架（如 Assistants API、Autogen、Langchain、SuperAGI 等）时最大的痛苦是什么？


我很想插话，但不幸的是，我不使用任何这些工具。我坚持使用 RAG 架构中的聊天完成 API，并使用 PHP 开发自己的解决方案。我正在创建由数千个不同大小、层次结构和语义差异的文档组成的知识库应用程序。以我的拙见，上述工具都无法解决在试图从法学硕士那里获得连贯、一致、全面、特别是廉价的答复时出现的无数问题。
但是，我在这里发帖是因为我也想知道到目前为止其他人的经历。]]></description>
      <guid>https://community.openai.com/t/biggest-pains-with-llm-agents-assistants-api-autogen-etc/578745#post_12</guid>
      <pubDate>Tue, 09 Jan 2024 00:01:01 GMT</pubDate>
    </item>
    <item>
      <title>Dall-E-3 不会生成 1024x1024 以外尺寸的图像</title>
      <link>https://community.openai.com/t/dall-e-3-wont-produce-image-in-sizes-other-than-1024x1024/578773#post_3</link>
      <description><![CDATA[需要查看不起作用的调用代码来提供帮助。但如果除了字面上只是用“1792x1024”替换大小变量之外完全相同，那么我不确定。]]></description>
      <guid>https://community.openai.com/t/dall-e-3-wont-produce-image-in-sizes-other-than-1024x1024/578773#post_3</guid>
      <pubDate>Mon, 08 Jan 2024 23:56:32 GMT</pubDate>
    </item>
    <item>
      <title>GPT 暂停用户上传的文件</title>
      <link>https://community.openai.com/t/gpts-halding-of-user-uploaded-file/578786#post_2</link>
      <description><![CDATA[您好，欢迎来到社区！
我确实浏览了论坛和现有插件，似乎没有简单的方法可以通过自定义 GPT 来处理此问题。
与 pdf 插件的聊天都会请求存储文件的 URL，然后将其上传到插件提供商的服务器。
在某些情况下，文件可以转换为 Base 64 并以块的形式发送到服务器，只是为了与服务器上的文件相似。
如果您的目标受众是日常用户，那么也许您可以创建一个操作，将一些动态分配给用户的网络空间以及一个使用最基本的用户界面上传文件的链接。然后，返回对话后，用户确认文件上传，对话就可以开始。]]></description>
      <guid>https://community.openai.com/t/gpts-halding-of-user-uploaded-file/578786#post_2</guid>
      <pubDate>Mon, 08 Jan 2024 23:52:47 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 为什么 API 调用这么慢？什么时候能修好？</title>
      <link>https://community.openai.com/t/openai-why-are-the-api-calls-so-slow-when-will-it-be-fixed/148339?page=6#post_102</link>
      <description><![CDATA[我还面临着聊天完成 API 的缓慢（用户感知的、与延迟相关的）问题，并且希望为这个主题和整体 OpenAI 开发者体验做出贡献。


《改善延迟》指南非常有用开始解决响应缓慢问题的有用地方


建议：如果 API 的completion_tokens/秒比率更加透明，那就太好了。有关速率限制和使用等级&lt;的文章/a&gt; 非常有用，但他们没有提到响应时间和每秒完成令牌的比率。如果此信息公开，其他开发商会感兴趣吗？也许在该文档中添加一个部分，列出每个层、每个区域的预期性能范围。


更进一步，我找不到一种投票方式来改进聊天完成 API 的性能（延迟）；该帖子已关闭，我恳请他们重新打开，以便我可以做出贡献。其他开发者是否也有兴趣对功能/文档请求进行投票？


&lt;小时/&gt;
近实时 Web 应用程序探索
我于 2023 年 12 月在第 1 层到第 3 层之间对不同版本的 GPT 进行了一些实验，第一个结论是，只有 gpt-3.5-turbo-1106 可以用于生产，以实现接近真实的效果时间应用程序，无论级别如何。这需要使用 流 或添加一些 UX 动画用户等待。
GPT 3.5 Turbo（2023 年 12 月 11 日开始）
我在 4 – 9 秒（平均 6 秒）内收到回复，回复率为 150 – 400 total_tokens/s&lt; /strong&gt;（平均~200）位于第 3 层；如果我们只考虑令牌完成（在延迟方面最重要的项目），我的比率是每秒 90 个令牌，平均520 个聊天完成令牌&lt; /strong&gt;
 → gpt-3.5-turbo-1106 的第 3 层性能稍好一些：

第 1 层平均响应时间为 10 秒
第 2 层平均响应时间为 9 秒

 → gpt-3.5-turbo-0613 的性能在第 1 层到第 3 层之间发生显着变化。
GPT-4 和 GPT-4-Turbo
它在 1-2 分钟内得到响应，这对于生产来说是不可接受的。


gpt-4-0613：我的平均响应时间为 1 分钟，比率为 36 total_tokens/s强&gt;


gpt-4-1106-preview：我的平均响应时间为 1.3 分钟，比率为 22 total_tokens/s 


 → 第 1 层到第 3 层之间没有明显差异。
&lt;小时/&gt;
注释
我收集了大约 100 个数据点，每个请求平均使用 1.2k 个令牌（0.7k 提示和 0.5k 完成）；我远低于我的 RPM、RPD、TPM 和 TPD 限制。我可以根据要求提供更多数据。]]></description>
      <guid>https://community.openai.com/t/openai-why-are-the-api-calls-so-slow-when-will-it-be-fixed/148339?page=6#post_102</guid>
      <pubDate>Mon, 08 Jan 2024 23:50:01 GMT</pubDate>
    </item>
    </channel>
</rss>