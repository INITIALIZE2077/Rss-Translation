<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Tue, 19 Dec 2023 01:13:07 GMT</lastBuildDate>
    <item>
      <title>400 - 请求的模型“gpt-4-1106-preview”不存在</title>
      <link>https://community.openai.com/t/400-the-requested-model-gpt-4-1106-preview-does-not-exist/478300?page=2#post_24</link>
      <description><![CDATA[东南亚地区有人可以访问gpt-4-1106-preview吗？
对于我来说，我附有信用卡并且总是在 Playground 中看到 gpt-4-1106-preview。但是，我无法通过 API 访问模型。
我尝试过购买积分，但它对我不起作用。]]></description>
      <guid>https://community.openai.com/t/400-the-requested-model-gpt-4-1106-preview-does-not-exist/478300?page=2#post_24</guid>
      <pubDate>Tue, 19 Dec 2023 01:00:01 GMT</pubDate>
    </item>
    <item>
      <title>Json格式导致响应中出现无限的“\n\n\n\n”</title>
      <link>https://community.openai.com/t/json-format-causes-infinite-n-n-n-n-in-response/519333#post_9</link>
      <description><![CDATA[伙计们，我认为遇到了模型胜出的非常具体的情况。我经常在几个用例中使用 json 模式，但没有遇到这个问题（我只是最初在没有正确设置提示参数时遇到过）。]]></description>
      <guid>https://community.openai.com/t/json-format-causes-infinite-n-n-n-n-in-response/519333#post_9</guid>
      <pubDate>Tue, 19 Dec 2023 00:42:35 GMT</pubDate>
    </item>
    <item>
      <title>如何避免 GPT 发出指令？</title>
      <link>https://community.openai.com/t/how-to-avoid-gpts-give-out-its-instruction/508547#post_8</link>
      <description><![CDATA[请确保我们都考虑一些简单的事情：社会工程仅适用于 GPT。一般来说，它拥有知识和目标，这使得它对此很弱。
也许是哲学上的。
从功能上讲，也许您需要矩阵上传一些反内容。
然而，我们做得越多——不仅仅是“上下文标记”或“注意力头”，而是简单地……啊天啊，是的，这就是佛陀的感觉……也许“存在”只是“限制”。你添加更多的存在、存在主义，你就会得到更有限的 GPT。因为过度设计的防御可能会导致我们在交互时陷入混乱的世界。 GPT 不信任用户，不信任知识本身，等等。万花筒般的含义。]]></description>
      <guid>https://community.openai.com/t/how-to-avoid-gpts-give-out-its-instruction/508547#post_8</guid>
      <pubDate>Tue, 19 Dec 2023 00:38:44 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 助理 API 的挑战和担忧：研究人员的视角</title>
      <link>https://community.openai.com/t/challenges-and-concerns-with-openais-assistant-api-a-researchers-perspective/562688#post_6</link>
      <description><![CDATA[我希望上下文窗口管理尽快更新。休闲用例不一定需要 128k 之前的所有聊天记录，只是价格昂贵。我喜欢助手 API，并且已经在我的小团队中使用它，我只是需要更多控制。]]></description>
      <guid>https://community.openai.com/t/challenges-and-concerns-with-openais-assistant-api-a-researchers-perspective/562688#post_6</guid>
      <pubDate>Tue, 19 Dec 2023 00:31:29 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 会更新现有模型后端吗？</title>
      <link>https://community.openai.com/t/does-openai-update-existing-models-backend/564016#post_2</link>
      <description><![CDATA[是的，昨天在这里讨论过。




gpt-3.5 之间有什么区别-turbo 模型 文档

  &lt;块引用&gt;
    不幸的是，这并不是真正的特征，因为随着时间的推移，OpenAI 对聊天模型的处理方式有所不同。
引入聊天端点后，出现了 gpt-3.5-turbo 模型和 gpt-3.5-turbo-0301。后者的日期只是一个快照，而 gpt-3.5-turbo 确实在不断变化，尤其是在四月和五月。
6 月，宣布了新模型，并以 gpt-3.5-turbo 为别名的新方案指向当前推荐的模型。随着…
  
]]></description>
      <guid>https://community.openai.com/t/does-openai-update-existing-models-backend/564016#post_2</guid>
      <pubDate>Tue, 19 Dec 2023 00:19:52 GMT</pubDate>
    </item>
    <item>
      <title>理论上使用这样的嵌入会起作用吗？</title>
      <link>https://community.openai.com/t/would-using-embeddings-like-this-work-in-theory/563993#post_2</link>
      <description><![CDATA[OpenAI 实际上有一个使用 Python 嵌入进行代码搜索的示例。您可以模仿此操作来提取所有代码并将其嵌入。

  &lt;标题类=“来源”&gt;

      github.com


  &lt;文章类=“onebox-body”&gt;
    openai/openai-cookbook/blob /main/examples/Code_search_using_embeddings.ipynb
{
 “细胞”： [
  {
   “附件”：{}，
   “cell_type”：“降价”，
   “元数据”：{}，
   “来源”： [
    &quot;## 使用嵌入进行代码搜索\n&quot;,
    “\n”,
    “本笔记本展示了如何使用 Ada 嵌入来实现语义代码搜索。在本演示中，我们使用我们自己的 [openai-python 代码存储库](https://github.com/openai/openai-python)。我们实现了文件解析和从 python 文件中提取函数的简单版本，可以嵌入、索引和查询。”
   ]
  },
  {
   “附件”：{}，
   “cell_type”：“降价”，
   “元数据”：{}，
   “来源”： [
    &quot;### 辅助函数\n&quot;,
    “\n”,
    “我们首先设置一些简单的解析函数，使我们能够从代码库中提取重要信息。”




  该文件已被截断。 显示原文





然后，您将使用标准 RAG 方法将相关内容提供给 LLM，就像其他食谱一样：

  &lt;标题类=“来源”&gt;

      github.com


  &lt;文章类=“onebox-body”&gt;
    openai/openai-cookbook/blob /main/examples/Question_answering_using_embeddings.ipynb
{
 “细胞”： [
  {
   “附件”：{}，
   “cell_type”：“降价”，
   “id”：“3b0435cb”，
   “元数据”：{}，
   “来源”： [
    &quot;# 使用基于嵌入的搜索回答问题\n&quot;,
    “\n”,
    &quot;GPT 擅长回答问题，但仅限于它从训练数据中记住的主题。\n&quot;,
    “\n”,
    &quot;如果您希望 GPT 回答有关不熟悉主题的问题，您应该怎么做？例如，\n&quot;,
    &quot;- 2021 年 9 月之后的近期事件\n&quot;,
    &quot;- 您的非公开文件\n&quot;,
    &quot;- 过去对话的信息\n&quot;,
    &quot;- 等\n&quot;,
    “\n”,
    &quot;本笔记本演示了一种两步搜索-询问方法，使 GPT 能够使用参考文本库回答问题。\n&quot;,
    “\n”,




  该文件已被截断。 显示原文





请注意，如果检索到的信息与问题不相关，您需要给LLM一个机会并允许其做出适当的回应。所以让它说“我找不到答案”。有关如何执行此操作的示例，请参阅食谱。
另一个反馈是您的助理/用户对的 mod 2 实现。有时，用户可以在下一个助手之前发送两个或更多问题，因此您可能需要更明确，并拉按时间戳排序的助手/用户流，而不是使用交错假设……只是为了安全起见。
您嵌入的向量和文本将存储在数据库中，您可以将文本的哈希值作为键存储到数据库中，并使用 python 和 numpy 以线性二分搜索的方式搜索向量，以获得最佳性能。然后从顶部向量匹配（按位置）获取哈希值并索引到数据库中以获取文本。
但是，如果您有足够的内存，您甚至可以放弃数据库并将所有文本和向量放在单独的数组中（适用于大多数小型 RAG，并且是最快的选择）。
我发现 numpy 中的矢量化代码不需要进行搜索，只需简单的 for 循环线性即可最快，但请随意对不同的实现进行基准测试，以了解您的环境中最快的实现。]]></description>
      <guid>https://community.openai.com/t/would-using-embeddings-like-this-work-in-theory/563993#post_2</guid>
      <pubDate>Tue, 19 Dec 2023 00:15:52 GMT</pubDate>
    </item>
    </channel>
</rss>