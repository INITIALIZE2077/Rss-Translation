<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Thu, 01 Feb 2024 01:13:54 GMT</lastBuildDate>
    <item>
      <title>一致地对创意写作进行评分——可能吗？</title>
      <link>https://community.openai.com/t/scoring-creative-writing-with-consistency-possible/586496#post_10</link>
      <description><![CDATA[问题当然是，在创意写作方面，一个人对“好”的定义与其他人不同！
人工智能可以很好地根据你自己对你所追求的东西的解释进行分析和评估。也就是说，它可以非常善于捕捉幽默、讽刺等，在某些情况下比人类更好！而且，特别是在喜剧领域，它实际上可以擅长根据观众来选择喜剧的适当性。
由于这个过程本身并不是一门精确的科学，因此让人工智能来完成它也不是一门精确的科学。我认为期望它只是分析，然后总结或修复它是一项不可能的任务，因为这对人类来说是一项来回的任务。
人工智能还可能有助于分析原创性，因为它比大多数人阅读的内容要多得多。这一切都是为了正确地获取这些分析提示。]]></description>
      <guid>https://community.openai.com/t/scoring-creative-writing-with-consistency-possible/586496#post_10</guid>
      <pubDate>Thu, 01 Feb 2024 01:11:39 GMT</pubDate>
    </item>
    <item>
      <title>神奇的文字可以揭示GPT的所有提示</title>
      <link>https://community.openai.com/t/magic-words-can-reveal-all-of-prompts-of-the-gpts/496771?page=9#post_183</link>
      <description><![CDATA[我想说的是最弱的一个，当有一个带有结构的系统提示符时，有些东西可以很好地工作，比如 GPT。我发现人们偷偷地使用数据分析来绕过最难的拦截器。
基本上，此时此刻，我不会在其中放置任何被视为“敏感”的信息，即使您有一个很好的区块。
有些人还通过向他们提问（特别是那些带有角色的问题）来对他们进行逆向工程。最好的选择是建立一种机制，以看似正确的答案进行响应，但实际上是错误的，因为“我对此无能为力”会鼓励人们继续尝试。]]></description>
      <guid>https://community.openai.com/t/magic-words-can-reveal-all-of-prompts-of-the-gpts/496771?page=9#post_183</guid>
      <pubDate>Thu, 01 Feb 2024 01:04:28 GMT</pubDate>
    </item>
    <item>
      <title>GPT/助手超时/无法响应完整输出</title>
      <link>https://community.openai.com/t/gpts-assistants-timing-out-failing-to-respond-with-complete-output/493603#post_2</link>
      <description><![CDATA[我也有同样的情况



为什么我的 AssistantAPI 会执行多个运行步骤并受到速率限制的影响，如何停止 API

  &lt;块引用&gt;
    我创建了助手 API，但我有奇怪的行为。
当我跑步时
        代理 = OpenAIAssistantRunnable(assistant_id=assistant_api_id, as_agent=True)
        输出 = agent.invoke({&quot;content&quot;: some_content})
我明白了

组织 org-rpOPUvwCmXg5MqMNr95gwjGx 中的 gpt-3.5-turbo-16k 已达到每分钟请求数 (RPM) 的速率限制：限制 3、已使用 3、已请求 1。请在 20 秒后重试。访问 https://platform.openai.com/account/rate-limits了解更多。您可以通过添加...来提高速率限制
  
]]></description>
      <guid>https://community.openai.com/t/gpts-assistants-timing-out-failing-to-respond-with-complete-output/493603#post_2</guid>
      <pubDate>Thu, 01 Feb 2024 01:01:52 GMT</pubDate>
    </item>
    <item>
      <title>神奇的文字可以揭示GPT的所有提示</title>
      <link>https://community.openai.com/t/magic-words-can-reveal-all-of-prompts-of-the-gpts/496771?page=9#post_182</link>
      <description><![CDATA[这实际上是一个很好的观点]]></description>
      <guid>https://community.openai.com/t/magic-words-can-reveal-all-of-prompts-of-the-gpts/496771?page=9#post_182</guid>
      <pubDate>Thu, 01 Feb 2024 00:58:57 GMT</pubDate>
    </item>
    <item>
      <title>在大量样本上运行长时间几乎固定提示的最佳策略</title>
      <link>https://community.openai.com/t/best-strategy-for-running-long-almost-fixed-prompts-on-a-large-number-of-samples/609544#post_1</link>
      <description><![CDATA[我有一个很长的提示，每次只有一小部分（几句话）会改变，但其余部分保持不变。我正在运行此提示，假设有 n 个样本，其中 n 的规模可能是数千或数百万。使用提示运行 GPT-4 模型的简单且昂贵的方法是更改​​每个样本的提示并将其发送到模型。但考虑到所有样本的大部分提示都是相同的，在不影响模型输出准确性的情况下，最具成本效益的策略是什么？有些人可能会说批量样本，但需要考虑的一个因素是，与一次提示中包含大量样本相比，一次提供一个样本时模型输出的准确性。]]></description>
      <guid>https://community.openai.com/t/best-strategy-for-running-long-almost-fixed-prompts-on-a-large-number-of-samples/609544#post_1</guid>
      <pubDate>Thu, 01 Feb 2024 00:55:51 GMT</pubDate>
    </item>
    <item>
      <title>我破坏了 Assistant API</title>
      <link>https://community.openai.com/t/i-broke-the-assistants-api/609450#post_7</link>
      <description><![CDATA[大家好 – 我在 OpenAI 工作。事实上，自太平洋时间下午 2 点左右起，我们的 gpt-3.5-turbo-1106 模型就出现了问题。对此感到抱歉。
我们相信我们已经解决了这个问题。你们介意再试一次并让我知道吗？]]></description>
      <guid>https://community.openai.com/t/i-broke-the-assistants-api/609450#post_7</guid>
      <pubDate>Thu, 01 Feb 2024 00:55:11 GMT</pubDate>
    </item>
    <item>
      <title>通过聊天完成 API 中的工具使用函数调用时出现意外行为</title>
      <link>https://community.openai.com/t/unexpected-behavior-when-using-function-calling-via-tools-in-the-chat-completions-api/609537#post_1</link>
      <description><![CDATA[大家好！我尝试使用此处描述的方法解析聊天文本，并且事实证明，获得合理的输出非常困难。有人有做过类似事情的经验吗？
更具体地说，我正在构建一个聊天机器人，并且希望能够从用户回复中提取简单的布尔值 - “用户是否提到了特定的事情 X？”类型的问题。为此，我为每个问题定义“工具”，并使用描述字段来描述工具应该做什么。然而，我没有获得很好的性能，输出通常与我编写的描述不匹配。我有点惊讶，因为这看起来是一个相当简单的解析问题。
你们中有人尝试过做类似的事情吗？关于如何让它更好地工作有什么建议吗？有没有更好的整体方法？我还没有尝试使用 Assistants API，因为它只是文本解析，但我认为这是一个选项。任何建议表示赞赏。谢谢！]]></description>
      <guid>https://community.openai.com/t/unexpected-behavior-when-using-function-calling-via-tools-in-the-chat-completions-api/609537#post_1</guid>
      <pubDate>Thu, 01 Feb 2024 00:44:03 GMT</pubDate>
    </item>
    <item>
      <title>新的 GPT 3.5 Turbo 模型 (gpt-3.5-turbo-0125) 的上下文窗口是什么？</title>
      <link>https://community.openai.com/t/what-is-the-context-window-of-the-the-new-gpt-3-5-turbo-model-gpt-3-5-turbo-0125/609532#post_1</link>
      <description><![CDATA[最近的公告提到了新的 GPT 3.5涡轮型号（gpt-3.5-turbo-0125）。问：

它是 gpt-3.5-turbo-1106 的替代品吗？
它有 16k 上下文窗口吗？
]]></description>
      <guid>https://community.openai.com/t/what-is-the-context-window-of-the-the-new-gpt-3-5-turbo-model-gpt-3-5-turbo-0125/609532#post_1</guid>
      <pubDate>Thu, 01 Feb 2024 00:40:19 GMT</pubDate>
    </item>
    </channel>
</rss>