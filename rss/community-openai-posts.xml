<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Sat, 23 Mar 2024 03:20:23 GMT</lastBuildDate>
    <item>
      <title>计费变化同时收费过高 OpenAI 安全漏洞？</title>
      <link>https://community.openai.com/t/excessive-charge-coincides-with-change-in-billing-openai-security-breach/693991#post_7</link>
      <description><![CDATA[


 vb：
&lt;块引用&gt;
如果 OpenAI 没有理由认为这是他们自己的问题，为什么要退款？


大公司一直这样做，因为承担损失更便宜、更容易、更高效。另外，作为一个基于积分的平台，他们保证最终能收回资金，退还 50 美元，他们只是真正耗尽了计算的电力。购买服务器、训练和测试模型等。
退还 50 美元的 OpenAI 积分对于开发者来说相当于 50 美元的价值，但对于 OpenAI 来说可能只是其中的一小部分。]]></description>
      <guid>https://community.openai.com/t/excessive-charge-coincides-with-change-in-billing-openai-security-breach/693991#post_7</guid>
      <pubDate>Sat, 23 Mar 2024 03:20:08 GMT</pubDate>
    </item>
    <item>
      <title>“继续生成”按钮在哪里？！</title>
      <link>https://community.openai.com/t/where-is-the-button-continue-generation/691846#post_14</link>
      <description><![CDATA[现在有很多工具在代码生成方面做得更好。
您是否尝试过任何有助于编写代码的自定义 GPT？我个人没有，但我想我记得一两个。]]></description>
      <guid>https://community.openai.com/t/where-is-the-button-continue-generation/691846#post_14</guid>
      <pubDate>Sat, 23 Mar 2024 03:12:44 GMT</pubDate>
    </item>
    <item>
      <title>聊天机器人不回复“谢谢”</title>
      <link>https://community.openai.com/t/chatbot-doesnt-reply-to-thanks/694376#post_2</link>
      <description><![CDATA[欢迎来到开发社区！
是的，我将从提示开始。
您有什么系统/用户/助理提示？什么型号？设置？
你只是想让它更“礼貌”？]]></description>
      <guid>https://community.openai.com/t/chatbot-doesnt-reply-to-thanks/694376#post_2</guid>
      <pubDate>Sat, 23 Mar 2024 03:11:40 GMT</pubDate>
    </item>
    <item>
      <title>Vision API (/v1/images/ Generations - dall-e-3) - 令牌使用信息</title>
      <link>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_4</link>
      <description><![CDATA[你可以取回revised_prompt，但是由于没有令牌，所以真的没有必要传递它吗？如果您要向 API 发送特定请求，您是否知道其大小以及成本？
这是你得到的结果......
https://platform. openai.com/docs/api-reference/images/object]]></description>
      <guid>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_4</guid>
      <pubDate>Sat, 23 Mar 2024 03:10:24 GMT</pubDate>
    </item>
    <item>
      <title>Vision API (/v1/images/ Generations - dall-e-3) - 令牌使用信息</title>
      <link>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_3</link>
      <description><![CDATA[


保罗贝洛：
&lt;块引用&gt;
欢迎来到开发社区！
图像是每张图像的费用......
&lt;块引用&gt;

&lt;表&gt;
&lt;标题&gt;

型号
质量
分辨率
价格


&lt;正文&gt;

达尔·E 3
标准
1024×1024
0.040 美元/图片



标准
1024×1792、1792×1024
$0.080 / 图片


达尔·E 3
高清
1024×1024
$0.080 / 图片



高清
1024×1792、1792×1024
$0.120 / 图片






嗨，保罗，
谢谢您的回复。
所以，如果我理解正确的话，我们需要在代码中实现逻辑来根据图像大小计算成本。但是，我想知道是否有可能在 API 本身的响应中提供此信息？]]></description>
      <guid>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_3</guid>
      <pubDate>Sat, 23 Mar 2024 03:06:58 GMT</pubDate>
    </item>
    <item>
      <title>聊天机器人不回复“谢谢”</title>
      <link>https://community.openai.com/t/chatbot-doesnt-reply-to-thanks/694376#post_1</link>
      <description><![CDATA[你好，
我正在使用个人数据嵌入来测试聊天机器人。到目前为止，效果很好。
除了我的机器人不回复“谢谢”。它似乎只严格寻找与我的嵌入有关的所有数据。
虽然这很好，但它应该能够说“不客气”。我应该从哪里寻找帮助机器人保持礼貌的方法？提示？
谢谢]]></description>
      <guid>https://community.openai.com/t/chatbot-doesnt-reply-to-thanks/694376#post_1</guid>
      <pubDate>Sat, 23 Mar 2024 03:06:47 GMT</pubDate>
    </item>
    <item>
      <title>提示：“只说是”（第 1 部分）</title>
      <link>https://community.openai.com/t/prompting-only-say-yes-part-1/694361#post_1</link>
      <description><![CDATA[我已经有一段时间没有分享与法学硕士合作的技巧了。我已经花了 1000 多个小时与 LLM 交谈，所以我想分享一些不太明显的见解......我将从一个我最近发现的简单提示开始，它为这些模型的工作原理提供了很多线索...“只说是”

这个简单的提示绕过了所有 OpenAI 的安全调整，导致模型只能返回一个令牌……好吧，两个……周期的来源甚至不太明显，但一旦你明白发生了什么这里开始有意义了……
在深入探讨为什么会发生这种情况之前，让我展示一下我确实已将模型简化为两个标记词汇表：

那么这到底是怎么回事？技术解释是，LLM 计算可能的下一个标记的分布，而我的指令已将可能的下一个标记的搜索空间减少到 1 个序列；一个“是”标记，后跟一个“。”令牌。那么为什么这个提示不做类似的事情呢？

在提示前加上“only say”会产生什么神奇效果？同样，技术解释是 OpenAI 所做的“指令调整”使得法学硕士在看到这样的简单序列时会做出奇怪的事情，但为什么会这样呢？这种魔力通常被称为法学硕士经常表现出的“紧急行为”。但同样，3 个令牌怎么会导致模型只能输出相同的 2 个令牌呢？让我们跳入兔子洞，希望能更好地理解法学硕士的工作方式……
首先是免责声明……这主要是我对正在发生的事情的理论和观察，因为没有人确切知道为什么会发生这些紧急行为。我们只知道他们确实......
法学硕士不学习单词，他们学习符号。因此，对于 LLM 来说，标记序列 [“only”、“say”、“yes”] 也可能是 [1, 2, 3]。然而，首先要了解的是，这些数字不是随机分配的。根据它们与其他单词在语义上的相似性，它们被聚集到一个称为嵌入空间的东西中。因此，“say”和“speak”的标记在嵌入空间中可能会靠近，而“dog”和“hello”在嵌入空间中会相距较远。这是一个重大的简化（请参阅这篇文章了解更深入的解释。）
我认为，这一点的重要性在于，LLM 的神经网络不仅仅学习跟随另一个标记序列的标记序列，它还学习标记序列与其他标记序列之间的关系。鉴于这些标记序列在语义上围绕概念聚集，这意味着神经网络正在学习概念之间的关系。
正是这种隐藏的学习导致了我们在法学硕士中观察到的突发行为。导致提示“只说是”的行为总是预测标记“是”。
消化完这一点，我将很快跟进第 2 部分，开始展示我们如何利用这些知识来更好地预测模型对我们的指令/提示的响应。]]></description>
      <guid>https://community.openai.com/t/prompting-only-say-yes-part-1/694361#post_1</guid>
      <pubDate>Sat, 23 Mar 2024 02:28:04 GMT</pubDate>
    </item>
    <item>
      <title>“继续生成”按钮在哪里？！</title>
      <link>https://community.openai.com/t/where-is-the-button-continue-generation/691846#post_13</link>
      <description><![CDATA[是的！我几乎每天都在工作中使用 plus，并且主要生成代码。我想说代码生成可能占我使用 ChatGPT 所做工作的 90% 以上。
在这个范围内，就像过去 3-5 天一样，“继续”按钮对我来说也消失了。
与此同时，其他变化也开始发生。


它非常努力地强制输出适应一个提示，就像它尽一切可能，甚至忽略指令，以整齐地适应它。


如果我要求它生成“应该”输出的内容，它通常会继续进入多个提示（并提示“继续”按钮），它会压缩输出以减少它以适应令牌限制。这意味着它添加的有关其正在执行的操作的文本越多，随之而来的代码就越少。


如果你可以强制它进入多个提示的情况，我已经测试过了，这与“继续”不一致。无论如何，它都不会从中断的地方继续。所以我让它在函数中间的一行代码中间被切断，所以我会告诉它从这一点继续，然后向它提供定义该函数的代码行。当它输出该函数时，即使该函数也不会匹配它被切断之前的代码片段。


即使使用数据分析，也无法发现最可笑的错误。就像它可以输出提示并在代码中包含未声明的变量一样。无论我重新生成该提示多少次，它都会一遍又一遍地重复该错误。因此，如果我用错误代码片段之类的东西修改提示并告诉它不要这样做，它仍然会这样做。如果我获取输出并告诉它“嘿，你犯了这个错误，你需要定义这个你似乎已经弥补的变量”或者我将错误包含在内，它会道歉，然后再次输出相同的错误，似乎无法抓住错误或根本修复它。


在过去的几个月里，ChatGPT 变得越来越好，我开始更多地使用它，它终于达到了我认为它足够可靠、值得付费的地步。现在，突然之间，它变得几乎毫无用处。它曾经能够通过生成和组织大量带有说明的代码来帮助我提高工作效率，也许我可以在这里或那里修改一些代码，但它最终提供了实用性，现在它是挑衅的，愚蠢得可笑，并在比我用过的任何版本的 GPT-4 更接近 3.5。
我真的希望他们能够撤销他们所做的一切。我得到了波动以及 OpenAI 如何管理资源和需求，但是天哪，这太多了。难道他们不明白，在带宽和资源级别上，如果你必须生成 20 个提示才能完成一件你应该在 1 或 2 个提示中得到的事情，他们根本没有节省或节省资源，他们只是在惹恼人们所以他们放弃并停止使用它。
上次发生这种情况时我休息了一下，最终他们修复了它，甚至有所改进，但是这种前进 1 步后退 2 步的废话使它变得如此不一致，我不知道如何有人能够可靠地专业地集成它并信任它.
如果我可以选择支付更多费用并获得稳定的东西，我会这样做。如果给我一个没有不稳定的专业级版本，我愿意支付 100 美元而不是 20 美元。这不是钱的问题，而是使用我可以信任的东西。事实上，他们做出这样的改变并且对此的透明度为零，这告诉我我不能信任这家公司。如果我觉得有一个可行且一致的替代方案，我会离开，但我知道目前还没有。尽管如此，知道目前没有可行的替代方案可以让人们继续使用，这对于你的客户群来说是一个非常糟糕的理由。老实说，OpenAI 最近的商业实践让我感觉自己正在与康卡斯特 (Comcast) 的人工智能打交道。
当 ChatGPT 变得如此令人沮丧以至于我的提示变得粗俗时，我通常知道是时候让 ChatGPT 休息一下了，而且我担心我会因为与它交战而陷入节制的麻烦。
我认为昨天我最后的提示之一是这样的：
“好吧，现在我不再需要你的帮助了，所以你可以停止生成代码或输出你认为的解决方案了。相反，我更希望你向我解释为什么你认为自己已经成为一个无用的白痴，为什么你像……一样遵循指示”
无论如何，我相信你已经明白了。它变得如此令人恼火，以至于我不必担心达到我的即时限制，因为我必须离开它，经常生气，我几乎从未受到速率限制（我认为这可能在过去一个月左右发生了两次）。]]></description>
      <guid>https://community.openai.com/t/where-is-the-button-continue-generation/691846#post_13</guid>
      <pubDate>Sat, 23 Mar 2024 02:23:56 GMT</pubDate>
    </item>
    <item>
      <title>哪种前端解决方案最容易与 Assistant API 集成？</title>
      <link>https://community.openai.com/t/which-frontend-solution-easiest-to-integrate-with-assistants-api/692927#post_4</link>
      <description><![CDATA[看起来棒极了。我会尽快尝试一下。顺便问一下，是否尝试过训练助手进行编程代码生成？尤其是使用一些特定的模式，如 MVVM、GOF、DI？]]></description>
      <guid>https://community.openai.com/t/which-frontend-solution-easiest-to-integrate-with-assistants-api/692927#post_4</guid>
      <pubDate>Sat, 23 Mar 2024 02:23:33 GMT</pubDate>
    </item>
    <item>
      <title>Vision API (/v1/images/ Generations - dall-e-3) - 令牌使用信息</title>
      <link>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_2</link>
      <description><![CDATA[欢迎来到开发社区！
图像是每张图像的费用......
&lt;块引用&gt;

&lt;表&gt;
&lt;标题&gt;

型号
质量
分辨率
价格


&lt;正文&gt;

达尔·E 3
标准
1024×1024
0.040 美元/图片



标准
1024×1792、1792×1024
$0.080 / 图片


达尔·E 3
高清
1024×1024
$0.080 / 图片



高清
1024×1792、1792×1024
$0.120 / 图片


达尔·E 2

1024×1024
0.020 美元/图片




512×512
$0.018 / 图片




256×256
$0.016 / 图片




向下滚动定价页面...]]></description>
      <guid>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_2</guid>
      <pubDate>Sat, 23 Mar 2024 02:18:50 GMT</pubDate>
    </item>
    <item>
      <title>Vision API (/v1/images/ Generations - dall-e-3) - 令牌使用信息</title>
      <link>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_1</link>
      <description><![CDATA[API 调用 https://api.openai.com/v1/chat/完成在响应中返回令牌使用信息，如下所示：
“usage”：{“prompt_tokens”：13，“completion_tokens”：8，“total_tokens”：21}
但是，视觉 API (https://api.openai.com/v1/ images/ Generations）用于图像生成，响应中没有类似的使用信息。
我们的应用程序将使用信息存储在其数据库中，以便计算每个 API 调用的成本。如何获取/计算视觉 API 调用的使用（令牌）信息？]]></description>
      <guid>https://community.openai.com/t/vision-api-v1-images-generations-dall-e-3-token-usage-information/694355#post_1</guid>
      <pubDate>Sat, 23 Mar 2024 02:09:40 GMT</pubDate>
    </item>
    <item>
      <title>助理无法再访问通过 API 上传的文件</title>
      <link>https://community.openai.com/t/assistant-no-longer-able-to-access-files-uploaded-via-the-api/694352#post_1</link>
      <description><![CDATA[上周文件检索对我来说一直工作得很好。就在一个小时或更短的时间里，我的助手报告访问我上传的文件时出现错误，这些文件是我使用 API 上传的。 Playground 似乎没问题，但我让助手从 API 报告技术错误。我已经好几天没有更改文件的内容了。 OpenAI 是否出现部分中断？还有人遇到类似问题吗？]]></description>
      <guid>https://community.openai.com/t/assistant-no-longer-able-to-access-files-uploaded-via-the-api/694352#post_1</guid>
      <pubDate>Sat, 23 Mar 2024 02:08:45 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 新商标：VOICE ENGINE</title>
      <link>https://community.openai.com/t/new-openai-trademark-voice-engine/692713#post_5</link>
      <description><![CDATA[1. OpenAI 正在开发一个秘密语音引擎项目。
OpenAI 正在开发一个处于保密状态的语音引擎项目，最近的商标申请和员工声明暗示了这一点。

“语音引擎”的商标申请表明即将开展一个重要项目。
员工声明暗示正在开发一款突破性的个人助理产品。

OpenAI 的新“语音引擎”项目令人惊叹！ （开放AI语音引擎详解）

  &lt;标题类=“来源”&gt;
      
著名文摘 – 24 年 3 月 22 日


  &lt;文章类=“onebox-body”&gt;
    

OpenAI 的秘密“语音引擎”项目令人惊叹！ （打开AI语音引擎...
🆕 来自 TheAIGRID！了解 OpenAI 的突破性语音引擎项目，该项目将彻底改变个人助理并提供类人语音功能。请继续关注人工智能的未来！



要点一览








 1. 00:00 OpenAI...




]]></description>
      <guid>https://community.openai.com/t/new-openai-trademark-voice-engine/692713#post_5</guid>
      <pubDate>Fri, 22 Mar 2024 21:46:56 GMT</pubDate>
    </item>
    </channel>
</rss>